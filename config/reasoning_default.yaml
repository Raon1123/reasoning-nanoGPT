DDP:
  backend: nccl
  gradient_accumulation_steps: 40

TRAIN:
  batch_size: 512
  block_size: 256
  max_iters: 50000

MODEL:
  model_type: nanoGPT
  config:
    n_layer: 12
    n_head: 12
    n_embd: 768
    dropout: 0.0
    bias: False
  compile: False
  resume: False

OPTIMIZER:
  optimizer_type: AdamW
  config:
    lr: 6.0e-4
    weight_decay: 1.0e-1
    betas: [0.9, 0.95]
  grad_clip: 1.0

SCHEDULER:
  scheduler_type: compose
  config:
    T_max: 50000
    eta_min: 6.0e-5

LOGGING:
  logger: wandb
  project: reasoning-nanoGPT
  run_name: test
  eval_interval: 2000
  log_interval: 1
  eval_only: False
  always_save_checkpoint: True
  checkpoint_dir: ./checkpoints
  max_checkpoints_to_keep: 5

DATASET:
  dataset_path: ./data/arc-aug-1000
